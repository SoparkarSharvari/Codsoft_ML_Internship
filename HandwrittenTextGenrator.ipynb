{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANDWRITTEN TEXT GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a character-level recurrent neural network (RNN) to generate handwritten-like text. Train the model on a dataset of handwritten text examples, and let it generate new text based on the learned patterns.\n",
    "\n",
    "> DataSet\n",
    "\n",
    "* https://paperswithcode.com/dataset/deepwriting\n",
    "* https://paperswithcode.com/dataset/iam\n",
    "* https://paperswithcode.com/dataset/hkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import random \n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir = \"/Users/sharvarisoparkar/Downloads/archive(3)/train\"\n",
    "train_data = []\n",
    "img_size = 32\n",
    "non_chars = [\"#\", \"$\", \"&\", \"@\"]\n",
    "for i in os.listdir(dir):\n",
    "    if i in non_chars:\n",
    "        continue\n",
    "    sub_directory = os.path.join(dir, i)\n",
    "    # Add a check to ensure it's a directory\n",
    "    if os.path.isdir(sub_directory):\n",
    "        count = 0\n",
    "\n",
    "        for j in os.listdir(sub_directory):\n",
    "  \n",
    "            count += 1\n",
    "            if count > 4000:\n",
    "                break\n",
    "            img = cv2.imread(os.path.join(sub_directory, j), 0)\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "            train_data.append([img, i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "val_dir = \"/Users/sharvarisoparkar/Downloads/archive(3)/Validation\"\n",
    "val_data = []\n",
    "img_size = 32\n",
    "non_chars = [\"#\", \"$\", \"&\", \"@\"]\n",
    "for i in os.listdir(val_dir):\n",
    "    if i in non_chars:\n",
    "        continue\n",
    "    sub_directory = os.path.join(val_dir, i)\n",
    "    if os.path.isdir(sub_directory):\n",
    "        count = 0\n",
    "        for j in os.listdir(sub_directory):\n",
    "            count += 1\n",
    "            if count > 1000:\n",
    "                break\n",
    "            img = cv2.imread(os.path.join(sub_directory, j), 0)\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "            val_data.append([img, i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15209"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random \n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_X = []\n",
    "train_Y = []\n",
    "for features,label in train_data:\n",
    "    train_X.append(features)\n",
    "    train_Y.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_X = []\n",
    "val_Y = []\n",
    "for features,label in val_data:\n",
    "    val_X.append(features)\n",
    "    val_Y.append(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "LB = LabelBinarizer()\n",
    "train_Y = LB.fit_transform(train_Y)\n",
    "val_Y = LB.fit_transform(val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_X = np.array(train_X)/255.0\n",
    "train_X = train_X.reshape(-1,32,32,1)\n",
    "train_Y = np.array(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_X = np.array(val_X)/255.0\n",
    "val_X = val_X.reshape(-1,32,32,1)\n",
    "val_Y = np.array(val_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 32, 32, 1) (15209, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(train_X.shape,val_X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 35) (15209, 35)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(train_Y.shape,val_Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(32,32,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(35, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 5, 5, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 35)                4515      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 162,851\n",
      "Trainable params: 162,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4375/4375 [==============================] - 134s 30ms/step - loss: 0.6096 - accuracy: 0.8069 - val_loss: 0.3443 - val_accuracy: 0.8847\n",
      "Epoch 2/50\n",
      "4375/4375 [==============================] - 146s 33ms/step - loss: 0.3446 - accuracy: 0.8849 - val_loss: 0.2769 - val_accuracy: 0.9144\n",
      "Epoch 3/50\n",
      "1924/4375 [============>.................] - ETA: 1:29 - loss: 0.3053 - accuracy: 0.8942"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_X,train_Y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data \u001b[38;5;241m=\u001b[39m (val_X, val_Y),  verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[1;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(train_X,train_Y, epochs=50, batch_size=32, validation_data = (val_X, val_Y),  verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Training Accuracy vs Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training Loss vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sort_contours(cnts, method=\"left-to-right\"):\n",
    "    reverse = False\n",
    "    i = 0\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "    key=lambda b:b[1][i], reverse=reverse))\n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (cnts, boundingBoxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_letters(img):\n",
    "    letters = []\n",
    "    image = cv2.imread(img)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret,thresh1 = cv2.threshold(gray ,127,255,cv2.THRESH_BINARY_INV)\n",
    "    dilated = cv2.dilate(thresh1, None, iterations=2)\n",
    "\n",
    "    cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        if cv2.contourArea(c) > 10:\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        roi = gray[y:y + h, x:x + w]\n",
    "        thresh = cv2.threshold(roi, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
    "        thresh = cv2.resize(thresh, (32, 32), interpolation = cv2.INTER_CUBIC)\n",
    "        thresh = thresh.astype(\"float32\") / 255.0\n",
    "        thresh = np.expand_dims(thresh, axis=-1)\n",
    "        thresh = thresh.reshape(1,32,32,1)\n",
    "        ypred = model.predict(thresh)\n",
    "        ypred = LB.inverse_transform(ypred)\n",
    "        [x] = ypred\n",
    "        letters.append(x)\n",
    "    return letters, image\n",
    "\n",
    "#plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_word(letter):\n",
    "    word = \"\".join(letter)\n",
    "    return word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 270ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "BEWE5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15568c6d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imutils\n",
    "\n",
    "letter,image = get_letters(\"/Users/sharvarisoparkar/Downloads/archive(5)/train_v2/train/TRAIN_00003.jpg\")\n",
    "word = get_word(letter)\n",
    "print(word)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.openai_secret_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_secret_manager\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai.openai_secret_manager'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import openai_secret_manager\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = openai_secret_manager.get_secret(\"openai\")[\"api_key\"]\n",
    "openai.api_key = api_key\n",
    "\n",
    "def generate_handwritten_image(text):\n",
    "    # Generate image using DALL-E\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"Generate an image of handwritten text: '{text}'\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    # Extract image URL from response\n",
    "    image_url = response[\"choices\"][0][\"image\"]\n",
    "\n",
    "    # Download the image\n",
    "    image_response = requests.get(image_url)\n",
    "    image = Image.open(BytesIO(image_response.content))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Get user input for the word\n",
    "user_word = input(\"Enter the word: \")\n",
    "\n",
    "# Generate handwritten image\n",
    "generate_handwritten_image(user_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
